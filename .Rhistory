# Source the integration script (which sources the other scripts)
source("second_pass/integration.R")
source("second_pass/welfare_analysis_framework.R")
# Run analysis with all four combinations of methods:
all_results <- run_all_welfare_method_combinations(
human_data_path = "dat/world_bank/world_bank_pop_gdp_clean.xlsx",
farmed_animal_data_path = "first_pass/calc_tseries.xlsx",
wild_animal_data_path = "second_pass/wild_calc_tseries.xlsx",
output_base_dir = "third_pass/welfare_results",
create_visualizations = TRUE
)
library(pacman)
p_load(tidyverse, dplyr, readr, ggplot2, gridExtra, png, mgcv, tidyselect,
stringr, readxl, openxlsx, foreign, broom, knitr, data.table, dlm)
# Source the integration script (which sources the other scripts)
source("second_pass/integration.R")
source("second_pass/welfare_analysis_framework.R")
# Run analysis with all four combinations of methods:
all_results <- run_all_welfare_method_combinations(
human_data_path = "dat/world_bank/world_bank_pop_gdp_clean.xlsx",
farmed_animal_data_path = "first_pass/calc_tseries.xlsx",
wild_animal_data_path = "second_pass/wild_calc_tseries.xlsx",
output_base_dir = "third_pass/welfare_results",
create_visualizations = TRUE
)
# Source the integration script (which sources the other scripts)
source("second_pass/integration.R")
source("second_pass/welfare_analysis_framework.R")
# Run analysis with all four combinations of methods:
all_results <- run_all_welfare_method_combinations(
human_data_path = "dat/world_bank/world_bank_pop_gdp_clean.xlsx",
farmed_animal_data_path = "first_pass/calc_tseries.xlsx",
wild_animal_data_path = "second_pass/wild_calc_tseries.xlsx",
output_base_dir = "third_pass/welfare_results",
create_visualizations = TRUE
)
# Source the integration script (which sources the other scripts)
source("second_pass/integration.R")
source("second_pass/welfare_analysis_framework.R")
# Run analysis with all four combinations of methods:
all_results <- run_all_welfare_method_combinations(
human_data_path = "dat/world_bank/world_bank_pop_gdp_clean.xlsx",
farmed_animal_data_path = "first_pass/calc_tseries.xlsx",
wild_animal_data_path = "second_pass/wild_calc_tseries.xlsx",
output_base_dir = "third_pass/welfare_results",
create_visualizations = TRUE
)
# Source the integration script (which sources the other scripts)
source("second_pass/integration.R")
source("second_pass/welfare_analysis_framework.R")
# Run analysis with all four combinations of methods:
all_results <- run_all_welfare_method_combinations(
human_data_path = "dat/world_bank/world_bank_pop_gdp_clean.xlsx",
farmed_animal_data_path = "first_pass/calc_tseries.xlsx",
wild_animal_data_path = "second_pass/wild_calc_tseries.xlsx",
output_base_dir = "third_pass/welfare_results",
create_visualizations = TRUE
)
# Filter out rows with NA values for WR utility
filtered_data_wr <- data %>%
filter(!is.na(WR_utility), !is.na(aliveatanytime))
# Source the integration script (which sources the other scripts)
source("second_pass/integration.R")
source("second_pass/welfare_analysis_framework.R")
# Run analysis with all four combinations of methods:
all_results <- run_all_welfare_method_combinations(
human_data_path = "dat/world_bank/world_bank_pop_gdp_clean.xlsx",
farmed_animal_data_path = "first_pass/calc_tseries.xlsx",
wild_animal_data_path = "second_pass/wild_calc_tseries.xlsx",
output_base_dir = "third_pass/welfare_results",
create_visualizations = TRUE
)
# Source the integration script (which sources the other scripts)
source("second_pass/integration.R")
source("second_pass/welfare_analysis_framework.R")
# Run analysis with all four combinations of methods:
all_results <- run_all_welfare_method_combinations(
human_data_path = "dat/world_bank/world_bank_pop_gdp_clean.xlsx",
farmed_animal_data_path = "first_pass/calc_tseries.xlsx",
wild_animal_data_path = "second_pass/wild_calc_tseries.xlsx",
output_base_dir = "third_pass/welfare_results",
create_visualizations = TRUE
)
# Source the integration script (which sources the other scripts)
source("second_pass/integration.R")
source("second_pass/welfare_analysis_framework.R")
# Run analysis with all four combinations of methods:
all_results <- run_all_welfare_method_combinations(
human_data_path = "dat/world_bank/world_bank_pop_gdp_clean.xlsx",
farmed_animal_data_path = "first_pass/calc_tseries.xlsx",
wild_animal_data_path = "second_pass/wild_calc_tseries.xlsx",
output_base_dir = "third_pass/welfare_results",
create_visualizations = TRUE
)
#Install necessary packages
# install.packages(c("tidyverse", "dplyr", "readr", "ggplot2", "gridExtra",
#                    "png", "mgcv", "tidyselect", "stringr", "readxl",
#                    "openxlsx", "foreign", "broom", "knitr", "data.table", "dlm"))
library(pacman)
p_load(tidyverse, dplyr, readr, ggplot2, gridExtra, png, mgcv, tidyselect,
stringr, readxl, openxlsx, foreign, broom, knitr, data.table, dlm)
# Source the integration script (which sources the other scripts)
source("second_pass/integration.R")
source("second_pass/welfare_analysis_framework.R")
# Run analysis with all four combinations of methods:
all_results <- run_all_welfare_method_combinations(
human_data_path = "dat/world_bank/world_bank_pop_gdp_clean.xlsx",
farmed_animal_data_path = "first_pass/calc_tseries.xlsx",
wild_animal_data_path = "second_pass/wild_calc_tseries.xlsx",
output_base_dir = "third_pass/welfare_results",
create_visualizations = TRUE
)
#' Extend animal population trends to match human time range using fitted models
#'
#' @param data The input dataset with all categories
#' @param target_year_range The target year range to extend to (e.g., 1950:2025)
#' @return Dataset with extended trends for all categories
extend_animal_trends <- function(data, target_year_range = 1960:2025) {
cat("Target extension range:", min(target_year_range), "to", max(target_year_range), "\n")
# Function to fit and predict trends for a single category
fit_and_extend_category <- function(category_data, category_name, target_years) {
if(nrow(category_data) < 3) {
warning(paste("Not enough data points for", category_name, "- skipping projection"))
return(category_data)
}
# Get the original year range
original_years <- range(category_data$Year)
cat("\n", category_name, "original range:", original_years[1], "to", original_years[2])
# If category already covers the full target range, return as-is
if(original_years[1] <= min(target_years) && original_years[2] >= max(target_years)) {
cat(" - already covers full range\n")
return(category_data)
}
# Determine years that need projection
years_to_project <- target_years[target_years < original_years[1] | target_years > original_years[2]]
if(length(years_to_project) == 0) {
cat(" - no projection needed\n")
return(category_data)
}
# Try different model types and choose the best fit
models <- list()
aic_values <- c()
# Linear model
tryCatch({
models$linear <- lm(aliveatanytime ~ Year, data = category_data)
aic_values <- c(aic_values, AIC(models$linear))
names(aic_values)[length(aic_values)] <- "linear"
}, error = function(e) {})
# Exponential model (log-transform dependent variable)
tryCatch({
if(all(category_data$aliveatanytime > 0)) {
models$exponential <- lm(log(aliveatanytime) ~ Year, data = category_data)
aic_values <- c(aic_values, AIC(models$exponential))
names(aic_values)[length(aic_values)] <- "exponential"
}
}, error = function(e) {})
# Polynomial model (degree 2)
tryCatch({
models$polynomial <- lm(aliveatanytime ~ poly(Year, 2), data = category_data)
aic_values <- c(aic_values, AIC(models$polynomial))
names(aic_values)[length(aic_values)] <- "polynomial"
}, error = function(e) {})
# Choose best model based on AIC
if(length(aic_values) == 0) {
warning(paste("No suitable model found for", category_name))
return(category_data)
}
best_model_name <- names(aic_values)[which.min(aic_values)]
best_model <- models[[best_model_name]]
cat(" - best model:", best_model_name, "(AIC:", round(min(aic_values), 2), ")")
# Generate predictions for missing years
prediction_data <- data.frame(Year = years_to_project)
if(best_model_name == "exponential") {
predicted_log_values <- predict(best_model, newdata = prediction_data)
predicted_values <- exp(predicted_log_values)
} else {
predicted_values <- predict(best_model, newdata = prediction_data)
}
# Ensure no negative predictions
predicted_values <- pmax(predicted_values, 0)
# Create extended data
extended_rows <- category_data[1, ] # Copy structure
extended_rows <- extended_rows[rep(1, length(years_to_project)), ]
extended_rows$Year <- years_to_project
extended_rows$aliveatanytime <- predicted_values
# Combine original and extended data
extended_data <- bind_rows(category_data, extended_rows) %>%
arrange(Year)
cat(" - extended by", length(years_to_project), "years\n")
return(extended_data)
}
# Process each category separately
extended_data <- data %>%
group_by(Category, Group) %>%
group_modify(~ fit_and_extend_category(.x, paste(.y$Category, .y$Group), target_year_range)) %>%
ungroup()
# Recalculate utility columns if they exist
if("WR_utility" %in% names(extended_data)) {
extended_data <- extended_data %>%
mutate(WR_utility = aliveatanytime * WR_potential * Welfare_level)
}
if("NC_utility" %in% names(extended_data)) {
extended_data <- extended_data %>%
mutate(NC_utility = aliveatanytime * NC_potential * Welfare_level)
}
if("NC_tot" %in% names(extended_data)) {
extended_data <- extended_data %>%
mutate(NC_tot = aliveatanytime * forebrain_neurons)
}
return(extended_data)
}
#' Visualize original vs extended trends for quality checking
#'
#' @param original_data Original dataset
#' @param extended_data Extended dataset
#' @param output_dir Directory to save comparison plots
create_trend_extension_plots <- function(original_data, extended_data, output_dir = "trend_extensions") {
if(!dir.exists(output_dir)) {
dir.create(output_dir, recursive = TRUE)
}
# Get categories that were extended
original_ranges <- original_data %>%
group_by(Category) %>%
summarize(min_year = min(Year), max_year = max(Year))
extended_ranges <- extended_data %>%
group_by(Category) %>%
summarize(min_year = min(Year), max_year = max(Year))
extended_categories <- original_ranges %>%
left_join(extended_ranges, by = "Category", suffix = c("_orig", "_ext")) %>%
filter(min_year_orig != min_year_ext | max_year_orig != max_year_ext) %>%
pull(Category)
# Create comparison plots for each extended category
for(category in extended_categories) {
orig_data <- original_data %>% filter(Category == category)
ext_data <- extended_data %>% filter(Category == category)
# Mark original vs extended data
plot_data <- bind_rows(
orig_data %>% mutate(Data_Type = "Original"),
ext_data %>% filter(!Year %in% orig_data$Year) %>% mutate(Data_Type = "Extended")
)
p <- ggplot(plot_data, aes(x = Year, y = aliveatanytime, color = Data_Type)) +
geom_line() +
geom_point(size = 0.8) +
scale_color_manual(values = c("Original" = "black", "Extended" = "red")) +
labs(title = paste("Trend Extension for", category),
y = "Population (alive at any time)",
x = "Year",
color = "Data Type") +
theme_minimal()
ggsave(file.path(output_dir, paste0(gsub("[^A-Za-z0-9]", "_", category), "_extension.pdf")),
plot = p, width = 10, height = 6)
}
cat("Trend extension plots saved to:", output_dir, "\n")
}
# Extend all trends to match human range
extended_data <- extend_animal_trends(integrated_data, target_year_range = 1950:2025)
# Read the integrated data from the saved file
integrated_data <- read_excel("third_pass/welfare_results/isoelastic/integrated_calc_tseries.xlsx")
# Then run the extension
extended_data <- extend_animal_trends(integrated_data, target_year_range = 1960:2025)
create_trend_extension_plots(integrated_data, extended_data, "trend_extension_plots")
#' Extend animal population trends to match human time range using local trend-aware methods
#'
#' @param data The input dataset with all categories
#' @param target_year_range The target year range to extend to (e.g., 1960:2025)
#' @param endpoint_years Number of years to use for endpoint trend calculation (default 5)
#' @return Dataset with extended trends for all categories
extend_animal_trends <- function(data, target_year_range = 1960:2025, endpoint_years = 5) {
cat("Target extension range:", min(target_year_range), "to", max(target_year_range), "\n")
# Function to fit and predict trends for a single category using local trends
fit_and_extend_category <- function(category_data, category_name, target_years, endpoint_years) {
if(nrow(category_data) < 3) {
warning(paste("Not enough data points for", category_name, "- skipping projection"))
return(category_data)
}
# Get the original year range
original_years <- range(category_data$Year)
cat("\n", category_name, "original range:", original_years[1], "to", original_years[2])
# If category already covers the full target range, return as-is
if(original_years[1] <= min(target_years) && original_years[2] >= max(target_years)) {
cat(" - already covers full range\n")
return(category_data)
}
# Sort data by year
category_data <- category_data %>% arrange(Year)
extended_data <- category_data
# BACKWARD EXTENSION (if needed)
years_before <- target_years[target_years < original_years[1]]
if(length(years_before) > 0) {
# Use first few years to establish backward trend
early_data <- category_data %>%
head(min(endpoint_years, nrow(category_data)))
if(nrow(early_data) >= 2) {
# Fit linear trend to early years
early_model <- lm(aliveatanytime ~ Year, data = early_data)
# Project backward
backward_predictions <- predict(early_model, newdata = data.frame(Year = years_before))
backward_predictions <- pmax(backward_predictions, 0)  # No negative values
# Create backward extension rows
backward_rows <- category_data[1, ][rep(1, length(years_before)), ]
backward_rows$Year <- years_before
backward_rows$aliveatanytime <- backward_predictions
extended_data <- bind_rows(backward_rows, extended_data)
cat(" - extended backward by", length(years_before), "years")
}
}
# FORWARD EXTENSION (if needed)
years_after <- target_years[target_years > original_years[2]]
if(length(years_after) > 0) {
# Use last few years to establish forward trend
recent_data <- category_data %>%
tail(min(endpoint_years, nrow(category_data)))
if(nrow(recent_data) >= 2) {
# Check if trend is relatively stable (low variability)
recent_cv <- sd(recent_data$aliveatanytime) / mean(recent_data$aliveatanytime)
if(recent_cv < 0.1) {
# If stable, use mean of recent years
forward_predictions <- rep(mean(recent_data$aliveatanytime), length(years_after))
cat(" - stable trend, using mean")
} else {
# If trending, fit linear trend to recent years
recent_model <- lm(aliveatanytime ~ Year, data = recent_data)
forward_predictions <- predict(recent_model, newdata = data.frame(Year = years_after))
forward_predictions <- pmax(forward_predictions, 0)  # No negative values
cat(" - trending, using linear extrapolation")
}
# Create forward extension rows
forward_rows <- category_data[nrow(category_data), ][rep(1, length(years_after)), ]
forward_rows$Year <- years_after
forward_rows$aliveatanytime <- forward_predictions
extended_data <- bind_rows(extended_data, forward_rows)
cat(" - extended forward by", length(years_after), "years")
}
}
# Sort final data
extended_data <- extended_data %>% arrange(Year)
cat("\n")
return(extended_data)
}
# Process each category separately
extended_data <- data %>%
group_by(Category, Group) %>%
group_modify(~ fit_and_extend_category(.x, paste(.y$Category, .y$Group), target_year_range, endpoint_years)) %>%
ungroup()
# Recalculate utility columns if they exist
if("WR_utility" %in% names(extended_data)) {
extended_data <- extended_data %>%
mutate(WR_utility = aliveatanytime * WR_potential * Welfare_level)
}
if("NC_utility" %in% names(extended_data)) {
extended_data <- extended_data %>%
mutate(NC_utility = aliveatanytime * NC_potential * Welfare_level)
}
if("NC_tot" %in% names(extended_data)) {
extended_data <- extended_data %>%
mutate(NC_tot = aliveatanytime * forebrain_neurons)
}
return(extended_data)
}
#' Advanced trend extension using LOESS smoothing (alternative approach)
#'
#' @param data The input dataset with all categories
#' @param target_year_range The target year range to extend to
#' @param span LOESS span parameter (0.2-0.8, lower = more local)
#' @return Dataset with extended trends using LOESS
extend_animal_trends_loess <- function(data, target_year_range = 1960:2025, span = 0.5) {
cat("Target extension range:", min(target_year_range), "to", max(target_year_range), "\n")
cat("Using LOESS smoothing with span =", span, "\n")
# Function to fit LOESS and extend
fit_loess_and_extend <- function(category_data, category_name, target_years, span) {
if(nrow(category_data) < 5) {
warning(paste("Not enough data points for LOESS on", category_name, "- using simple method"))
return(category_data)
}
original_years <- range(category_data$Year)
cat("\n", category_name, "original range:", original_years[1], "to", original_years[2])
if(original_years[1] <= min(target_years) && original_years[2] >= max(target_years)) {
cat(" - already covers full range\n")
return(category_data)
}
# Fit LOESS model
tryCatch({
loess_model <- loess(aliveatanytime ~ Year, data = category_data, span = span)
# For extensions, we'll use linear extrapolation from the LOESS endpoints
# rather than extrapolating the LOESS itself (which can be unstable)
extended_data <- category_data
# Get LOESS predictions at the endpoints
start_pred <- predict(loess_model, newdata = data.frame(Year = original_years[1]))
end_pred <- predict(loess_model, newdata = data.frame(Year = original_years[2]))
# Backward extension using linear trend from start
years_before <- target_years[target_years < original_years[1]]
if(length(years_before) > 0) {
# Use first few actual points to estimate initial trend
first_few <- category_data %>% head(3)
if(nrow(first_few) >= 2) {
early_slope <- (first_few$aliveatanytime[nrow(first_few)] - first_few$aliveatanytime[1]) /
(first_few$Year[nrow(first_few)] - first_few$Year[1])
backward_predictions <- start_pred + early_slope * (years_before - original_years[1])
backward_predictions <- pmax(backward_predictions, 0)
backward_rows <- category_data[1, ][rep(1, length(years_before)), ]
backward_rows$Year <- years_before
backward_rows$aliveatanytime <- backward_predictions
extended_data <- bind_rows(backward_rows, extended_data)
cat(" - extended backward by", length(years_before), "years")
}
}
# Forward extension using linear trend from end
years_after <- target_years[target_years > original_years[2]]
if(length(years_after) > 0) {
# Use last few actual points to estimate final trend
last_few <- category_data %>% tail(3)
if(nrow(last_few) >= 2) {
recent_slope <- (last_few$aliveatanytime[nrow(last_few)] - last_few$aliveatanytime[1]) /
(last_few$Year[nrow(last_few)] - last_few$Year[1])
# Dampen extreme slopes for stability
if(abs(recent_slope) > abs(end_pred * 0.1)) {
recent_slope <- sign(recent_slope) * abs(end_pred * 0.1)
cat(" - dampened extreme slope")
}
forward_predictions <- end_pred + recent_slope * (years_after - original_years[2])
forward_predictions <- pmax(forward_predictions, 0)
forward_rows <- category_data[nrow(category_data), ][rep(1, length(years_after)), ]
forward_rows$Year <- years_after
forward_rows$aliveatanytime <- forward_predictions
extended_data <- bind_rows(extended_data, forward_rows)
cat(" - extended forward by", length(years_after), "years")
}
}
extended_data <- extended_data %>% arrange(Year)
cat("\n")
return(extended_data)
}, error = function(e) {
warning(paste("LOESS failed for", category_name, "- using fallback method"))
return(category_data)
})
}
# Process each category separately
extended_data <- data %>%
group_by(Category, Group) %>%
group_modify(~ fit_loess_and_extend(.x, paste(.y$Category, .y$Group), target_year_range, span)) %>%
ungroup()
# Recalculate utility columns
if("WR_utility" %in% names(extended_data)) {
extended_data <- extended_data %>%
mutate(WR_utility = aliveatanytime * WR_potential * Welfare_level)
}
if("NC_utility" %in% names(extended_data)) {
extended_data <- extended_data %>%
mutate(NC_utility = aliveatanytime * NC_potential * Welfare_level)
}
if("NC_tot" %in% names(extended_data)) {
extended_data <- extended_data %>%
mutate(NC_tot = aliveatanytime * forebrain_neurons)
}
return(extended_data)
}
#' Visualize original vs extended trends for quality checking
#'
#' @param original_data Original dataset
#' @param extended_data Extended dataset
#' @param output_dir Directory to save comparison plots
create_trend_extension_plots <- function(original_data, extended_data, output_dir = "trend_extensions") {
if(!dir.exists(output_dir)) {
dir.create(output_dir, recursive = TRUE)
}
# Get categories that were extended
original_ranges <- original_data %>%
group_by(Category) %>%
summarize(min_year = min(Year), max_year = max(Year))
extended_ranges <- extended_data %>%
group_by(Category) %>%
summarize(min_year = min(Year), max_year = max(Year))
extended_categories <- original_ranges %>%
left_join(extended_ranges, by = "Category", suffix = c("_orig", "_ext")) %>%
filter(min_year_orig != min_year_ext | max_year_orig != max_year_ext) %>%
pull(Category)
# Create comparison plots for each extended category
for(category in extended_categories) {
orig_data <- original_data %>% filter(Category == category)
ext_data <- extended_data %>% filter(Category == category)
# Mark original vs extended data
plot_data <- bind_rows(
orig_data %>% mutate(Data_Type = "Original"),
ext_data %>% filter(!Year %in% orig_data$Year) %>% mutate(Data_Type = "Extended")
)
# Add connecting points at boundaries for visual continuity
if(nrow(plot_data %>% filter(Data_Type == "Extended")) > 0) {
boundary_years <- c(min(orig_data$Year), max(orig_data$Year))
boundary_data <- orig_data %>%
filter(Year %in% boundary_years) %>%
mutate(Data_Type = "Boundary")
plot_data <- bind_rows(plot_data, boundary_data)
}
p <- ggplot(plot_data, aes(x = Year, y = aliveatanytime, color = Data_Type)) +
geom_line(size = 0.8) +
geom_point(size = 1.2, alpha = 0.7) +
scale_color_manual(values = c("Original" = "black", "Extended" = "red", "Boundary" = "blue")) +
labs(title = paste("Trend Extension for", category),
y = "Population (alive at any time)",
x = "Year",
color = "Data Type") +
theme_minimal() +
theme(legend.position = "bottom")
ggsave(file.path(output_dir, paste0(gsub("[^A-Za-z0-9]", "_", category), "_extension.pdf")),
plot = p, width = 10, height = 6)
}
cat("Trend extension plots saved to:", output_dir, "\n")
}
extended_data <- extend_animal_trends(integrated_data, target_year_range = 1960:2025, endpoint_years = 5)
create_trend_extension_plots(integrated_data, extended_data, "trend_extension_plots")
extended_data <- extend_animal_trends_loess(integrated_data, target_year_range = 1960:2025, span = 0.5)
create_trend_extension_plots(integrated_data, extended_data, "trend_extension_plots2")
